#!/usr/bin/env bash

set -eu

cd ~/m

cluster_name=
infra_pod_cidr=
infra_service_cidr=

function make_init {
	sudo docker rm -f registry
	sudo rm -rf /tmp/certs
	mkdir -p /tmp/certs
	(cd /tmp/certs && step certificate create --subtle --insecure --no-password --force --profile self-signed $(uname -n) $(uname -n).crt $(uname -n).key)
	sudo docker run -d -p 5000:5000 --restart=always --name registry \
		-v /tmp/certs:/certs \
		-e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/$(uname -n).crt \
		-e REGISTRY_HTTP_TLS_KEY=/certs/$(uname -n).key \
			registry:2 || true
	
	#app repo add cache.defn.run:5000 --type helm --name cache --enable-oci --insecure-skip-server-verification
}

function make_linkerd {
	# CA
	step certificate create root.linkerd.cluster.local ca.crt ca.key --profile root-ca --no-password --insecure

	# signing cluster
	step certificate create identity.linkerd.cluster.local cluster.crt cluster.key --profile intermediate-ca --not-after 8760h --no-password --insecure --ca ca.crt --ca-key ca.key

	# install linkerd cluster
	linkerd --context ${cluster_name} install --crds | k --context ${cluster_name} apply -f -
	linkerd --context ${cluster_name} install --identity-trust-anchors-file ca.crt --identity-issuer-certificate-file cluster.crt --identity-issuer-key-file cluster.key | k --context ${cluster_name} apply -f -
	linkerd --context ${cluster_name} check

	# install viz cluster
	linkerd --context ${cluster_name} viz install --set dashboard.enforcedHostRegexp=^ | k --context ${cluster_name} apply -f -

	# install mc cluster
	linkerd --context ${cluster_name} multicluster install --gateway=false | k --context ${cluster_name} apply -f -
	linkerd --context ${cluster_name} multicluster check
}

function make_down {
	sudo ip link delete cilium_host || true
	sudo ip link delete cilium_net || true
	sudo ip link delete cilium_vxlan || true

	sudo iptables-save | grep -iv cilium | sudo iptables-restore
	sudo ip6tables-save | grep -iv cilium | sudo ip6tables-restore

	/usr/local/bin/k3s-uninstall.sh || true
}

function make_argocd {
	kubectl config use-context ${cluster_name}

	(
		cp $HOME/.kube/config $HOME/.kube/config.argocd
		export KUBECONFIG=$HOME/.kube/config.argocd
		eval "$(kubectl config view -o json | jq -r --arg cluster ${cluster_name} --arg namespace argocd '.contexts[] | select(.name == $cluster).context | "kubectl config set-context \(.cluster)-argocd --cluster=\(.cluster) --user=\(.user) --namespace=\($namespace)"')"
		kubectl config use-context ${cluster_name}-argocd
	)
}

function make_up {
	rm -vf ~/.kube/config

	set +x
	local TOKEN="$(curl -sSL -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")"
	local container_ip="$(curl -sSL -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/local-ipv4)"
	set -x
	sudo mkdir -p /etc/rancher/k3s
	sudo cp registries.yaml /etc/rancher/k3s/
	k3sup install --local --context ${cluster_name} --local-path ~/.kube/config --merge \
		--k3s-version "$(cd c/${cluster_name} && cue export --out json -e class.infra_k3s_version | jq -r . | cut -d: -f2 | sed 's#-#+#')" \
		--k3s-extra-args "--node-ip "${container_ip}" --disable-network-policy --disable=traefik --flannel-backend=none --kube-apiserver-arg=--service-account-issuer=$(cd c/${cluster_name} && cue export --out json -e class.discovery_url | jq -r .)/openid --kube-apiserver-arg=--api-audiences=https://kubernetes.default.svc.cluster.local,sts.amazonaws.com --tls-san=${cluster_name} --cluster-cidr=${infra_pod_cidr} --service-cidr=${infra_service_cidr} --kubelet-arg=eviction-hard=memory.available<500Mi -kubelet-arg=eviction-hard=nodefs.available<5%" 

	# oidc
	(cd c/${cluster_name} && make secrets)

	kubectl --context ${cluster_name} apply --server-side -f k/crd/${cluster_name}.yaml
	kubectl --context ${cluster_name} apply -f k/r/${cluster_name}-cluster-cilium/main.yaml
	kubectl --context ${cluster_name} create ns argocd || true
	kubectl --context ${cluster_name} apply -f k/r/${cluster_name}-cluster-argo-cd/main.yaml

	make_argocd

	# switch back to the default config
	kubectl config use-context ${cluster_name}

	env KUBECONFIG=$HOME/.kube/config.argocd kubectl -n argocd rollout status deployment/argocd-server
	env KUBECONFIG=$HOME/.kube/config.argocd kubectl -n argocd rollout status deployment/argocd-repo-server

	# patch argocd admin password
	env KUBECONFIG=$HOME/.kube/config.argocd kubectl patch secret argocd-secret -p '{"stringData": { "admin.password": "'$(htpasswd -bnBC 10 "" admin | tr -d ':\n')'"}}'

	# add k3s to argocd
	while true; do
		if env KUBECONFIG=$HOME/.kube/config.argocd argocd cluster add ${cluster_name} --yes --core --in-cluster --kube-context ${cluster_name}-argocd --name ${cluster_name}-cluster --upsert; then
			break
		fi
		sleep 5
	done

	# deploy the cluster app-of-apps
	kubectl --context ${cluster_name} apply -f e/${cluster_name}-cluster.yaml
	(cd c && make update-env env=$(echo ${cluster_name} | cut -d- -f3-))
}

function main {
	local cmd
	cmd="$1"
	shift

	case "${cmd}" in
		init)
			true
			;;
		*)
			if [[ -n "${1:-}" ]]; then
				cluster_name="$1"
				shift
			else
				cluster_name="$(cd ~/m/c/$(uname -n)  && cue eval -e class.cluster_name | jq -r .)"
			fi
			infra_pod_cidr="$(cd ~/m/c/${cluster_name} && cue eval -e class.infra_pod_cidr | jq -r .)"
			infra_service_cidr="$(cd ~/m/c/${cluster_name} && cue eval -e class.infra_service_cidr | jq -r .)"
			;;
	esac
			
	"make_${cmd}"
}

time main "$@"
uptime

# archive
function make_vc {
	for v in $(cd c/${cluster_name} && cue export --out json -e infra.parent.vclusters | jq -r '.[]'); do
		i="${v#vc}"
		# wait until vcluster is ready
		while true; do
			if vcluster --context ${cluster_name} connect vcluster -n "${cluster_name}-${v}-vcluster" --server=$(kubectl get --context ${cluster_name} -n ${cluster_name}-${v}-vcluster svc vcluster -o json | jq -r '.spec.clusterIP'):443 --update-current=false --kube-config-context-name ${cluster_name}-"${v}" --kube-config "$HOME"/.kube/config."${v}"; then
				break
			fi
			sleep 5
		done

		# add vcluster to argocd
		while true; do
			if env KUBECONFIG="$HOME/.kube/config.argocd:$HOME/.kube/config.${v}" argocd --kube-context ${cluster_name}-argocd cluster --core add "${cluster_name}-${v}" --yes; then break; fi
			sleep 5
		done
	done
}
