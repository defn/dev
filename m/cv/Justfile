# Computer Vision Gallery Management System - Justfile
# ================================================
#
# This Justfile provides a comprehensive set of tasks for managing a computer vision
# image gallery system. It handles image processing, metadata management, web serving,
# and integration with external services like ESRGAN and Replicate AI.
#
# Key Components:
# - Image processing pipeline (thumbnails, upscaling, format conversion)
# - Web gallery generation with blurhash placeholders
# - Database management for image metadata
# - User content downloading and processing
# - ESRGAN integration for image super-resolution
# - Backup and synchronization workflows
# - Development server management
#
# Usage Examples:
#   just gallery                    # Generate image gallery
#   just user username              # Download user content and generate thumbnails
#   just ersgan 5                   # Run ESRGAN on 5 parallel processes
#   just backup 20                  # Backup with 20 transfer threads
#
# Directory Structure:
#   img/         - Original full-size images
#   thumbs/      - Thumbnail versions (400px)
#   t2/          - Processed images for web display
#   replicate/   - ESRGAN enhanced images
#   posts/       - JSON metadata files
#   yes/no/      - Image classification directories

set shell := ["bash", "-cu"]

# End-of-day cleanup and reset operations
# Processes JSON files, cleans up directories, and truncates large files
eod:
    just json
    ls -d js-*.js.json | runmany 8 'echo $1; just posts $1 $1.txt'
    rm -rf no
    mkdir -p no
    rm -rf replicate/js
    mkdir -p replicate/js
    find thumbs t2 img yodayo/dl -type f -size +100c | runmany 64 'echo $1; truncate $1 --size 0'


# Aggregate all JSON metadata files into a single file and extract URLs
# Combines post metadata and user data, then generates a deduplicated URL list
json file="js.json":
    find posts -name '*.json' | while read -r js; do echo; cat $js; echo; done > {{ file }}
    for a in js-*js.json; do echo; cat $a; echo; done >> {{ file }}
    cat {{ file }} | jq -r '.url' | sort -u >> urls.txt
    cat urls.txt | sort -u > urls.txt.2
    mv urls.txt.2 urls.txt
    rm -f {{ file }}

posts input file:
    #!/usr/bin/env bash
    for i in {{ input }}; do
        cat "$i" | jq -r '"\(.id) \(.url) \(@base64)"' > {{ file }}.txt
        cat {{ file }}.txt | awk '{print $1, $2}' | while read -r post url; do img="${url##*/}"; echo $img; done | sort | while read -r img; do if test -f yes/$img; then if ! test -f posts/${img%.jpeg}.json; then echo posts/$img; grep "/$img " {{ file }}.txt | awk '{print $3}' | base64 -d > posts/${img%.jpeg}.json; fi; fi; done
    done
    rm {{ file }}.txt

# Download and process content for a specific user (using blurmap.go)
# Fetches user data, processes JSON, and downloads associated images
user uname stage1="yes" stage2="no":
    echo "{{uname}}" | go run blurmap.go -mode user -users /dev/stdin
    go run blurmap.go -mode download
    reset

t2:
    cat <(find yes -type f | cut -d/ -f2 | cut -d. -f1) <(find yes -type f | cut -d/ -f2 | cut -d. -f1) <(find t2 -name '*.png' | cut -d/ -f2 | cut -d. -f1) | sort | uniq -c | grep ' 2 ' | awk '{print $NF}' | sort | runmany 32 1 'echo "img/$1.jpeg -> t2/$1.png"; mogrify -path t2 -resize 400x -format png img/$1.jpeg'

# Generate all w-* workflow galleries in batch
# Processes all pub/fm/w-* directories and generates chunked galleries in pub/w/
# Uses navigate mode for clickable links to detail pages
w-galleries:
    go run blurmap.go -mode batch

# Generate web gallery with blurhash placeholders
# Creates responsive HTML gallery from processed images in t2 directory
# Uses Go program to generate blurhash previews for fast loading
# Now scans directory directly without needing all.input file
gallery prefix="replicate/":
    go run blurmap.go -mode gallery -scan-dir {{prefix}}t2

# Generate curation gallery from uncurated images
# Creates state/todo.txt with uncurated images and generates gallery in tmp/g
todo:
    go run blurmap.go -mode todo
    go run blurmap.go -mode gallery -file-list state/todo.txt
    echo 1 > LAST

daily file="following.txt" n="8":
    go run blurmap.go -mode download

download-user file n="4":
    go run blurmap.go -mode user -users {{ file }}

# Start backup process in screen session
# Syncs replicate directory to remote storage with configurable transfer threads
backup transfers="60":
    screen -dmS backup j backup-inner {{ transfers }}
    screen -DRR backup

backup-inner transfers:
    rm -rf replicate/js
    mkdir -p replicate/js
    rclone sync -L --progress --ignore-checksum --transfers={{ transfers }} replicate district-enc:cv/replicate

# Start nginx proxy server in screen session
# Serves static files and handles web requests for the gallery
proxy:
    screen -dmS proxy j proxy-inner
    screen -DRR proxy

proxy-inner:
    cat "$(pwd)/proxy.conf" | perl -pe "s{.*}{root $(pwd);} if m{^\s*root }" > /tmp/nginx.conf
    nginx -g "daemon off;" -c /tmp/nginx.conf

server:
    screen -dmS server j server-inner
    screen -DRR server

server-inner:
    while true; do etc/static; date; sleep 10; done

reset:
    screen -dmS reset j reset-inner
    screen -DRR reset

reset-inner:
    just eod
    rm -f js-* js.json.* js.json || true

# Quality assurance checks for data consistency
# Identifies overlaps, missing files, and inconsistencies between directories
qa:
    @echo Finding overlap between yes and no
    @for a in yes yes no; do find $a -type f; done | cut -d/ -f2- | cut -d. -f1 | sort | uniq -c | grep ' 3 ' || true
    @echo Finding unique between replicate/t2 and replicate/img
    @for a in replicate/t2 replicate/img; do find $a -type f; done | perl -pe 's{.*/}{}' | cut -d. -f1 | sort | uniq -c | grep ' 1 ' || true
    @echo Finding replicate/t2 but not in yes
    @for a in yes yes replicate/t2; do find $a -type f; done | perl -pe 's{.*/}{}' | cut -d. -f1 | sort | uniq -c | grep ' 1 ' || true
    @echo Finding yes but not in replicate/t2
    @for a in yes yes replicate/t2; do find $a -type f; done | perl -pe 's{.*/}{}' | cut -d. -f1 | sort | uniq -c | grep ' 2 ' || true

send from snap disk:
    sudo zfs send -i {{from}}@$(zfs list -t snapshot -o name -s creation -r {{disk}}/cv | tail -n 1 | cut -d@ -f2) {{from}}@{{snap}} | pv | sudo zfs receive {{disk}}/cv

split dataset from to where="~/work/backup/zfs/cv-":
    sudo zfs send -i {{dataset}}@{{from}} {{dataset}}@{{to}} | pv | split -b 5G --numeric-suffixes=1 --suffix-length=2 - {{where}}{{to}}.zfs.
    for a in {{where}}{{to}}.zfs.??; do echo "$a"; cat "$a" | sha256sum > "$a.sha256"; done

# Open curation galleries in browser using Tailscale IP
# Uses LAST file to track progress through gallery list
open chunk="1":
    #!/usr/bin/env bash
    # Get tailscale IP
    ts_ip=$(tailscale ip -4)

    # Generate sorted list of gallery directories
    galleries=$(ls -d tmp/g/*/ 2>/dev/null | sort -V)

    if [ -z "$galleries" ]; then
        echo "No galleries found in tmp/g/"
        exit 0
    fi

    # Get total number of galleries
    total=$(echo "$galleries" | wc -l)

    # Get starting position from LAST (default to 1)
    start=$(cat LAST 2>/dev/null || echo "1")

    chunk_size={{chunk}}

    echo "Found $total galleries, starting at position $start"

    # Process in chunks
    for ((i=start; i<=total; i+=chunk_size)); do
        # Open chunk of galleries
        echo "$galleries" | sed -n "${i},$((i + chunk_size - 1))p" | while read -r path; do
            url="http://${ts_ip}:8111/${path}index.html"
            echo "Opening: $url"
            open "$url"
            sleep 0.1
        done

        # Update LAST
        next=$((i + chunk_size))
        echo "$next" > LAST

        exit

        if [ $next -le $total ]; then
            read -p "Press Enter to continue at position $next..."
        fi
    done

    echo "All galleries opened"

freeze from snap backups:
    #!/usr/bin/env bash
    sudo zfs snapshot fast/cv@{{snap}} || true
    if test -n "{{backups}}"; then runmany 'just send fast/cv {{snap}} $1' {{backups}}; fi
    if test -z "{{backups}}"; then just split fast/cv {{from}} {{snap}} /backup/backup/zfs/cv-; fi

link:
    ln -nvfs ~/m/cv/{Justfile,*.mjs,proxy.conf,package.json,etc,gallery.*,tutorial.html,.joyride} .

v-all type n="1":
    ls v/mp4/*.mp4 | cut -d/ -f3 | sed 's#.mp4$##' | runmany {{n}} 'just v-{{type}} $1'

v-split vid:
    mkdir -p v/img/{{vid}}
    name={{vid}}; cd v/mp4 && ffmpeg -i "$name.mp4" -vf "fps=1/1" ../img/$name/v-$name-%04d.jpeg

v-html vid:
    name={{vid}}; find v/img/$name -type f | sed 's#^v/img/##; s#.jpeg$##' | sort | ./etc/cols.sh yes img /thumbs v/ img jpeg > v/html/$name.html

v-gen type="html" n="2":
    (cd v/img; ls -d */ | cut -d/ -f1) | runmany {{n}} 'just v-{{type}} $1'
